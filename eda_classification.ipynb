{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSSIデータを用いた場所分類\n",
    "\n",
    "`data/` ディレクトリにあるRSSIデータ（ファイル名に場所が含まれる）を読み込み、1分間のRSSIデータパターンから場所を分類するモデルを構築します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "# import joblib # model_utils で使用\n",
    "import japanize_matplotlib\n",
    "\n",
    "# --- ライブラリからのインポート ---\n",
    "from blelocation.feature_engineering import extract_inference_features as extract_features_lib # 名前が衝突するため別名でインポート\n",
    "from blelocation.model_utils import save_model_bundle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データの読み込みと前処理\n",
    "\n",
    "`data/` ディレクトリ内の全CSVファイルを読み込み、ファイル名からクラスラベル（場所）を抽出します。\n",
    "その後、絶対時刻をdatetime型に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data')\n",
    "all_files = list(data_dir.glob('*.csv'))\n",
    "\n",
    "all_dfs = []\n",
    "for f in all_files:\n",
    "    # ファイル名からクラスラベル（場所）を抽出 (例: 202504261747_クローゼット.csv -> クローゼット)\n",
    "    match = re.search(r'_(.+)\\.csv', f.name)\n",
    "    if match:\n",
    "        location = match.group(1)\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            # datetime列が存在し、内容があるか確認\n",
    "            if 'datetime' in df.columns and not df['datetime'].isnull().all():\n",
    "                df['location'] = location\n",
    "                # datetime列をdatetime型に変換 (エラーは無視してNaTにする)\n",
    "                df['datetime'] = pd.to_datetime(df['datetime'], errors='coerce')\n",
    "                # time_s, rssi_dbm列が存在するか確認\n",
    "                if 'time_s' in df.columns and 'rssi_dbm' in df.columns:\n",
    "                     # NaTになった行を削除\n",
    "                    df.dropna(subset=['datetime'], inplace=True)\n",
    "                    if not df.empty:\n",
    "                       all_dfs.append(df[['datetime', 'rssi_dbm', 'location']])\n",
    "                    else:\n",
    "                        print(f\"[WARN] No valid datetime entries in {f.name} after conversion.\")\n",
    "                else:\n",
    "                    print(f\"[WARN] 'time_s' or 'rssi_dbm' column missing in {f.name}. Skipping.\")\n",
    "            else:\n",
    "                 print(f\"[WARN] 'datetime' column missing or empty in {f.name}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to read or process {f.name}: {e}\")\n",
    "    else:\n",
    "        print(f\"[WARN] Could not extract location from filename: {f.name}. Skipping.\")\n",
    "\n",
    "if not all_dfs:\n",
    "    print(\"[ERROR] No valid data found in CSV files. Exiting.\")\n",
    "    # ここで処理を中断するか、空のDataFrameを作成するかを決定\n",
    "    # exit() # スクリプトの場合\n",
    "    data = pd.DataFrame(columns=['datetime', 'rssi_dbm', 'location'])\n",
    "else:\n",
    "    data = pd.concat(all_dfs, ignore_index=True)\n",
    "    # datetimeでソート\n",
    "    data = data.sort_values(by='datetime').reset_index(drop=True)\n",
    "\n",
    "print(f\"Total records loaded: {len(data)}\")\n",
    "print(\"Unique locations:\", data['location'].unique())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 データのサンプリング\n",
    "\n",
    "データを1分ごとのウィンドウに分割し、各ウィンドウに **MIN_POINTS_PER_WINDOW** 以上のデータポイントが含まれるものを1サンプルとします。\n",
    "各サンプルから特徴量（RSSIの平均、標準偏差、最小値、最大値など）を抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプリングウィンドウ内の最小データポイント数\n",
    "MIN_POINTS_PER_WINDOW = 5\n",
    "\n",
    "# datetimeをインデックスに設定\n",
    "if not data.empty and 'datetime' in data.columns:\n",
    "    data.set_index('datetime', inplace=True)\n",
    "else:\n",
    "    print(\"[WARN] Data is empty or 'datetime' column missing before resampling.\")\n",
    "\n",
    "# 1分ごとにリサンプリングし、各ウィンドウで特徴量を計算\n",
    "# Grouperを使って1分間のウィンドウを作成\n",
    "# def extract_features(window): ... Function definition removed ...\n",
    "\n",
    "if not data.empty:\n",
    "    # location ごとにグループ化してからリサンプリングしないと、異なる場所のデータが混ざる可能性がある\n",
    "    sampled_features_list = []\n",
    "    for location, group in data.groupby('location'):\n",
    "        # 1分間のウィンドウでリサンプリングし、各ウィンドウに関数を適用\n",
    "        # 'T' は分単位のオフセットエイリアス\n",
    "        resampler = group.resample('1min')\n",
    "        # ライブラリ関数を使用 (lambdaで追加引数を渡す)\n",
    "        # features = resampler.apply(lambda window: extract_features_lib(window, rssi_col='rssi_dbm', feature_names=None, min_points_required=MIN_POINTS_PER_WINDOW))\n",
    "        features = [extract_features_lib(window, rssi_col='rssi_dbm', feature_names=None, min_points_required=MIN_POINTS_PER_WINDOW) for _,window in resampler]\n",
    "        features = pd.concat(features, ignore_index=True).assign(location=location)\n",
    "        # 欠損値（データが閾値未満だったウィンドウ）を削除\n",
    "        features.dropna(inplace=True)\n",
    "        if not features.empty:\n",
    "           sampled_features_list.append(features)\n",
    "\n",
    "    if sampled_features_list:\n",
    "        sampled_data = pd.concat(sampled_features_list)\n",
    "        # stdがNaNになる場合（ウィンドウ内のデータが1つの場合など）を0で埋める\n",
    "        sampled_data['rssi_std'].fillna(0, inplace=True)\n",
    "        print(f\"Total samples after resampling (>={MIN_POINTS_PER_WINDOW} points/min): {len(sampled_data)}\")\n",
    "        print(\"Sample counts per location:\")\n",
    "        print(sampled_data['location'].value_counts())\n",
    "        sampled_data.head()\n",
    "    else:\n",
    "       print(f\"[WARN] No samples met the criteria (>={MIN_POINTS_PER_WINDOW} points/min).\")\n",
    "       sampled_data = pd.DataFrame() # 空のDataFrame\n",
    "else:\n",
    "    print(\"[WARN] Data is empty, skipping feature extraction.\")\n",
    "    sampled_data = pd.DataFrame() # 空のDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA (探索的データ分析)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_dataが空でないかチェック\n",
    "if not sampled_data.empty:\n",
    "    # 特徴量の分布を確認 (数値型の特徴量のみ)\n",
    "    numeric_features = sampled_data.select_dtypes(include=np.number).columns\n",
    "    sampled_data[numeric_features].hist(bins=15, figsize=(15, 10), layout=(-1, 3))\n",
    "    plt.suptitle('Distribution of Features')\n",
    "    plt.tight_layout(rect=(0, 0.03, 1, 0.95)) # Adjust layout to prevent title overlap\n",
    "    plt.show()\n",
    "\n",
    "    # クラスごとの特徴量の比較 (箱ひげ図)\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    for i, feature in enumerate(numeric_features):\n",
    "        if feature != 'count': # countはサンプル数なので除外することが多い\n",
    "           plt.subplot(2, 3, i + 1) # レイアウト調整\n",
    "           sns.boxplot(x='location', y=feature, data=sampled_data)\n",
    "           plt.title(f'{feature} by Location')\n",
    "           plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 特徴量間の相関\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = sampled_data[numeric_features].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"[INFO] sampled_data is empty. Skipping EDA plots.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. モデル構築と評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_dataが空でないかチェック\n",
    "if not sampled_data.empty:\n",
    "    # 特徴量 (X) とターゲット (y) に分割\n",
    "    X = sampled_data.drop(['location'], axis=1)\n",
    "    y = sampled_data['location']\n",
    "\n",
    "    # データセットをトレーニングセットとテストセットに分割\n",
    "    # クラスの比率を保つために stratify=y を指定\n",
    "    # 十分なサンプルがないクラスがあるとエラーになる可能性あり\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "        print(f\"Train set size: {X_train.shape[0]}\")\n",
    "        print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "        # 特徴量のスケーリング\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # ランダムフォレストモデルのトレーニング\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # テストデータで予測\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        # モデルの評価\n",
    "        print(\"\\nModel Evaluation:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        # 混同行列の計算\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "        cm_abs = cm # 絶対数を保持\n",
    "\n",
    "        # Precision (列方向で正規化) を計算 (ゼロ除算を避ける)\n",
    "        cm_sum_col = cm.sum(axis=0, keepdims=True)\n",
    "        # ゼロ除算が発生する列（合計が0）は NaN になるようにする\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            cm_precision = cm.astype('float') / cm_sum_col\n",
    "            cm_precision[np.isnan(cm_precision)] = 0 # NaN は 0 に置換\n",
    "\n",
    "        # Precisionで色付けし、絶対数値を表示\n",
    "        plt.figure(figsize=(10, 8)) # サイズ調整\n",
    "        sns.heatmap(cm_precision, annot=cm_abs, fmt='d', cmap='Blues',\n",
    "                    xticklabels=model.classes_.tolist(), yticklabels=model.classes_.tolist(),\n",
    "                    cbar_kws={'label': 'Precision (Column Normalized)'}) # カラーバーにラベル追加\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix (Color by Precision, Number is Count)')\n",
    "        plt.show()\n",
    "\n",
    "        # 特徴量の重要度\n",
    "        feature_importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=feature_importances, y=feature_importances.index)\n",
    "        plt.title('Feature Importances')\n",
    "        plt.xlabel('Importance Score')\n",
    "        plt.ylabel('Features')\n",
    "        plt.show()\n",
    "\n",
    "        # --- モデル、スケーラー、メタデータをまとめて保存 ---\n",
    "        bundle_filename = 'location_classification_bundle.joblib'\n",
    "        # MIN_POINTS_PER_WINDOW はこのセルより前のセル（サンプリング部）で定義されている想定\n",
    "        bundle = {\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'classes': model.classes_,\n",
    "            'features': X.columns.tolist(), # Indexをリストに変換\n",
    "            'min_points_per_window': MIN_POINTS_PER_WINDOW\n",
    "        }\n",
    "        # ライブラリ関数で保存\n",
    "        save_model_bundle(bundle, bundle_filename)\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"\\n[ERROR] Could not split data or train model: {e}\")\n",
    "        print(\"This might be due to insufficient samples in one or more classes for stratification.\")\n",
    "    except NameError as e:\n",
    "        # MIN_POINTS_PER_WINDOW が定義されていない場合のエラーハンドリング\n",
    "        if 'MIN_POINTS_PER_WINDOW' in str(e):\n",
    "            print(f\"\\n[ERROR] {e}. Please make sure the cell defining MIN_POINTS_PER_WINDOW (in section 2.1) is executed before this cell.\")\n",
    "        else:\n",
    "            print(f\"\\n[ERROR] An unexpected NameError occurred: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"[INFO] sampled_data is empty. Skipping model training and evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
